---
title: "Data Embedding"
author: "Taylor Arnold"
output:
  pdf_document: default
  html_document: default
---

```{r, message = FALSE, warning = FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(methods)
library(stringi)
library(keras)
library(xgboost)
```

So, the firs thing that we wanted to do was go ahead and read in our model from resnet50. We are using this model because it is extremely deep with many dozens of layers and millions of tunable parameters. It will provide a strong model for our neural network. 

```{r}
resnet50 <- application_resnet50(weights = 'imagenet', include_top = TRUE)
model_embed <- keras_model(inputs = resnet50$input,
                           outputs = get_layer(resnet50, 'avg_pool')$output)
```


Next, we wanted to pick a corpus that was both broad, with many different classes, and with many images per class. The Caltech-UCSD Birds image set provided this with 200 different classes of birds from the Black Footed Albatross to the common Cardinal. This image set was interesting to work with due to this large depth of variability in both color and size and provided a unique opportunity to test our neural network against a large class set with many common traits from class to class. Additionally, withe 200 classes, any model that chose the right bird above a probability of 0.005 is better than a random guess.

```{r}
input_dir <- "Birds"

image_paths <- dir(input_dir, recursive = TRUE)
ext <- stri_match(image_paths, regex = "\\.([A-Za-z]+$)")[,2]
image_paths <- image_paths[stri_trans_tolower(ext) %in% c("jpg", "png", "jpeg")]

print(sprintf("You have a total of %d images in the corpus.", length(image_paths)))
```

Myself, being a programming novice, wanted to ensure that the image path to get the Birds pictures was working. So, I experimented around with mapping to an arbitrary yellow breasted chat. 
```{r}
image_path <- "Birds/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0102_21696.jpg"
image <- image_load(image_path, target_size = c(224,224))
image <- image_to_array(image)
image <- array_reshape(image, c(1, dim(image)))
dim(image)
par(mar = rep(0, 4L))
plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE, type = "n", asp=1)
rasterImage(image[1,,,] / 255,0,0,1,1)
```
Nothing special here, we just went ahead and extracted the images into the class_vector.

```{r}
# you may need to change this:
class_vector <- dirname(image_paths)

# check that the classes look correct (should show
# all of the class names and how many images are in
# in each class)
cbind(table(class_vector))
```

Once again, we are just partitioning our data into 60% training data and 40% validation data. We tried a couple other values but things became either over or under fit on the training data which resulted in worse predictions. Additionally, we assumed everyone else would stick to a 60:40 split so we opted against being trendy for the sake of an apples to apples comparison with our fellow classmates. 

```{r}
# shuffle the input in a consistent way
set.seed(1)
if (length(class_vector) != length(image_paths)) stop("Something is very wrong!")
index <- sample(seq_along(class_vector))
image_paths <- image_paths[index]
class_vector <- class_vector[index]

# create training ids (this makes a 60/40 split, change 0.6 to modify this)
# it uses some fancier logic to make sure that the split is even
set.seed(1)
class_num <- as.numeric(factor(class_vector)) - 1L
vals <- runif(length(class_num))
coffs <- tapply(vals, class_num, quantile, probs = 0.6)
train_id <- if_else(vals <= coffs[class_num + 1], "train", "valid")

# create metadata dataset
img_data <- tibble(obs_id = sprintf("id_%06d", seq_along(class_vector)),
                   train_id = train_id,
                   class = class_num,
                   class_name = class_vector,
                   path_to_image = file.path(input_dir, image_paths))

# save the dataset as a csv file
write_csv(img_data, "my-image-data.csv")

# print out table of training and validation samples in each class
table(img_data$class_name, img_data$train_id)
```

Unfortunately, my computer had problems embedding such a large image set. It either claimed to have embedded the whole set when in reality it did not or it got to the very last batch, finished, and then I would get the pinwheel of death until I was had to force quit the program. So, Dr. Arnold graciously embedded it for us on his more powerful computer and gave us the .rds file that we needed. We included the lines of code that we WOULD have run if he had not done so.

```{r}
#num_cols <- model_embed$output_shape[[length(model_embed$output_shape)]]
#X <- matrix(NA_real_, nrow = nrow(img_data), ncol = as.numeric(num_cols))
```

Now, simply cycle through batches of the data, embed each batch, and save each batch
in X.

#```{r}
 this will load the data in 5 batches; make the number large
 enough so that you do not run into memory issues
num_batch <- 5
batch_id <- sample(seq_len(num_batch), nrow(img_data), replace=TRUE)

input_shape <- unlist(model_embed$input_shape)[1:2]
for (j in seq_len(num_batch))
{
  print(sprintf("Processing batch %d of %d", j, num_batch))
  these <- which(batch_id == j)
  unlist(model_embed$input_shape)

  Z <- array(0, dim = c(length(these), input_shape, 3))
  for (i in seq_along(these))
  {
    pt <- img_data$path_to_image[these[i]]
    image <- image_to_array(image_load(pt, target_size = input_shape))
    Z[i,,,] <- array_reshape(image, c(1, c(input_shape, 3)))
  }
  X_temp <- predict(model_embed, x = imagenet_preprocess_input(Z), verbose = TRUE)
  X[these,] <- array(X_temp, dim = c(length(these), ncol(X)))
}
#```

Then, you can save X as binary file on your computer:

#```{r}
write_rds(X, "my-image-embed.rds")
#```

This is where we loaded in Dr. Arnold's .rds file. It worked like a charm.
```{r}
X <- read_rds("my-image-embed.rds")
```

## Sanity check

Now that we have the embedding it was time to run our neural network. We first took our image data and created our X_train and y_train components. The y_train is a one hot encoding of the classes indicating which class that it is. 

```{r}
X_train <- X[img_data$train_id == "train",]
y_train <- to_categorical(img_data$class[img_data$train_id == "train"])
```

We experimented with several different models. The first model that we created had only 256 units with 2 layers. Despite giving good results (~27% accuracy) we wanted to try out other models. We first added another layer to our network and noticed that the accuracy actually went down and it also took longer to run. So this proved to be a bad strategy. We tried adding another layer (4 in total) and this also decreased accuracy. We were surprised by this but we went back to our two layer model for future model creation. We then increased the number of units in our neural network from 256 to 512, noting that our accuracy increased. So, we decided to increase the number of units to 1024 which gave us our best results of ~63% accuracy. We felt that this gave a good trade off between computing time and accuracy. The data was generally overfit and matched the training data with an accuracy of about 99%. However, the 63% accuracy that we accomplished is well above anything else that we predicted which was favorable.
```{r}
num_units <- 1024

model <- keras_model_sequential()
model %>%
  layer_dense(units = num_units, input_shape = ncol(X_train)) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%

  layer_dense(units = num_units) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  
  layer_dense(units = ncol(y_train)) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(lr = 0.001 / 2),
                  metrics = c('accuracy'))

history <- model %>%
  fit(X_train, y_train, epochs = 25)
y_pred <- predict_classes(model, X)
tapply(img_data$class == y_pred, img_data$train_id, mean)
```

Get probabilities and names:

```{r}
y_probs <- predict(model, X)
y_pred <- apply(y_probs, 1, which.max) - 1L
y_pred_name <- sort(unique(class_vector))[y_pred + 1L]
```

Confusion Matrix

The confusion matrix showed that we generally had good resuls. However, as the number of classes was so large, we did not have the screen space to display the entire matrix. So, although lots of data is shown, only the first part of the matrix shows relevant data. While we were overwhelming accurate at predicting the class of the bird as noted by the large numbers on the diagonal, the occasional misclassifications added up and resulted in our ~37% misclassification rate. 
```{r}
library(forcats)
lvl <- sort(unique(img_data$class_name))
class_name <- unique(img_data$class_name)
table(value = img_data$class_name, y_pred, train_id)
```

The Birbiest Birbs

At this point we wanted to look at which birds were most respresentational of its class. In order to do this, we took the maximum probability of correct classification in each class and displayed them. These represent the strongest correlation between image and class that our model predicted. 
```{r}
# Most
id <- apply(y_probs, 2, which.max)
ids <- id
par(mfrow = c(3, 5))
for (i in id) {
  par(mar = rep(0, 4L))
  plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE,type = "n", asp=1)
  Z <- image_load(img_data$path_to_image[i],
                  target_size = c(224,224))
  Z <- image_to_array(Z)
  rasterImage(Z/255, 0, 0, 1, 1)
  text(0.5, 0.1, label = img_data$class_name[i], col = "red", cex=1)
  text(0.5, 0.2, label = y_pred_name[i], col = "white", cex=1)
}
```

Visualizing the disparity of classes

At this point we wanted to look at how distinguished the classes were in our model. Although this is hard to see from the graphic below due to the large number of classes, the generally disparate nature of our classes should be noted. 
```{r}
library(irlba)

pca <- as_tibble(prcomp_irlba(X)$x[,1:2])
pca$y <- y_pred_name
```
```{r}
ggplot(pca, aes(PC1, PC2)) +
  geom_point(aes(color = y), alpha = 1, size = 1, show.legend = FALSE) +
  labs(x = "", y = "", color = "class") +
  theme_minimal()

library(dplyr)
pca %>%
  group_by(y) %>%
  summarize(PC1 = mean(PC1), PC2 = mean(PC2)) %>%
  ggplot(aes(PC1, PC2)) +
    geom_text(aes(label = y), size = 2)+
    #geom_point(aes(color = y), alpha = 1, size = 1, show.legend = FALSE) +
    labs(x = "", y = "", color = "class") +
    theme_void()

plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE,type = "n", asp=1)
  Z <- image_load(img_data$path_to_image[ids[134]],
                  target_size = c(224,224))
  Z <- image_to_array(Z)
  rasterImage(Z/255, 0, 0, 1, 1)
  
plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE,type = "n", asp=1)
  Z <- image_load(img_data$path_to_image[ids[85]],
                  target_size = c(224,224))
  Z <- image_to_array(Z)
  rasterImage(Z/255, 0, 0, 1, 1)
```



Model 2: XG Boosted Trees

This model did not work well at all! We wanted to try some other non-neural network models as an experiment. This is because gradient boosted trees are not good at deliniating between abstracts. In response to this, we decided to go ahead and try a KNN.
```{r}
positionNumber <- seq(200)
X_train <- X[img_data$train_id == "train",]
y_train <- to_categorical(img_data$class[img_data$train_id == "train"]) %*% positionNumber

y_train <- y_train / 200

y_valid <- to_categorical(img_data$class[img_data$train_id == "valid"]) %*% positionNumber
X_valid <- X[img_data$train_id == "valid",]

y_valid <- y_valid / 200

```
```{r}
model <- xgboost(data = X_train, label = y_train,
                 max_depth = 15, eta = .09, nthread = 4,
                 nrounds = 20, objective = "reg:linear",
                 verbose = 1, print_every_n=10)

pred <- round(predict(model, newdata = X))
tapply(img_data$class == pred, img_data$train_id, mean)
```

Model 3: KNN

So, we decided to go ahead and see how a KNN model works as they generally have a better time of distinguishing between pictures. However, this did not go so well as we only achieved a classification rate of ~20%. Although much worse than a neural network, this KNN model still performs significantly better than a random guess. 
```{r}
X_train <- X[img_data$train_id == "train",]
y_train <- to_categorical(img_data$class[img_data$train_id == "train"])


y_valid <- to_categorical(img_data$class[img_data$train_id == "valid"])
X_valid <- X[img_data$train_id == "valid",]

y_probs <- predict(model, X)

```

```{r}
library(FNN)

y_train <- img_data$class[img_data$train_id == "train"]

for (k in c(3, 10, 25, 50, 100))
{
  pred_valid <- FNN::knn(X_train, X_valid, y_train, k=k) #$pred
  print(mean(pred_valid == y_valid))
}


table(pred_valid)

```


